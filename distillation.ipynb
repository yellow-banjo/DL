{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71afda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34929c9",
   "metadata": {},
   "source": [
    "Попробуем применить метод дистилляции на примере задачи классификации. В качестве датасета возьмем CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163635f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "t = transforms.ToTensor()\n",
    "\n",
    "t = transforms.Compose([transforms.ToTensor(), \n",
    "                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=True, transform=t)\n",
    "train_loader = DataLoader(cifar_train, batch_size=batch_size, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "cifar_test = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=False, transform=t)\n",
    "test_loader = DataLoader(cifar_test, batch_size=batch_size, shuffle=False, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "classes = ('plane', 'car' , 'bird',\n",
    "    'cat', 'deer', 'dog',\n",
    "    'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8398155",
   "metadata": {},
   "source": [
    "В качестве учителя возьмем большущую VGG16 с батчнормализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe56ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "teacher = models.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd64343",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.classifier[6] = torch.nn.Linear(4096, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55e4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_train(teacher, train=True):\n",
    "    if train:\n",
    "        teacher.load_state_dict(torch.load('teacher_weights'))\n",
    "    else:\n",
    "        epochs=10\n",
    "        optimizer = torch.optim.SGD(teacher.parameters(), lr = 1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "        loss_f = nn.CrossEntropyLoss()\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        accuracy = []\n",
    "        teacher.to(device)\n",
    "        for i in tqdm(range(epochs)):\n",
    "            #Train\n",
    "            loss_mean = 0\n",
    "            elements = 0\n",
    "            for X, y in train_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = teacher(X)\n",
    "                loss = loss_f(y_pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_mean += loss.item() * len(X)\n",
    "                elements += len(X)\n",
    "\n",
    "            train_losses.append(loss_mean / elements)\n",
    "            #Test\n",
    "            if (i+1) % 1 == 0:\n",
    "                loss_mean = 0 \n",
    "                elements = 0\n",
    "                correct = 0\n",
    "                for X, y in iter(test_loader):\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "                    y_pred = teacher(X)\n",
    "                    loss = loss_f(y_pred, y)\n",
    "                    loss_mean += loss.item() * len(X)\n",
    "                    elements += len(X)\n",
    "                    y_pred = torch.argmax(y_pred, dim=1)\n",
    "                    correct += sum(y_pred == y).item()\n",
    "                accuracy.append(100 * correct / elements)\n",
    "                test_losses.append(loss_mean / elements)\n",
    "                print(\"Epoch\", i+1, \"| Train loss\", train_losses[-1], \"| Test loss\", test_losses[-1], \"| accuracy\", accuracy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9600118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(teacher.state_dict(), 'teacher_weights')\n",
    "#teacher.load_state_dict(torch.load('teacher_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1132cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_train(teacher, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc7beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model):\n",
    "    #функция выводит количество параметров у поданной на вход модели\n",
    "    return sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b7ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, loader):\n",
    "    model = model.cpu()\n",
    "    with torch.no_grad():\n",
    "        elements = 0\n",
    "        correct = 0\n",
    "        for X, y in iter(loader):\n",
    "            elements += len(X)\n",
    "            y_pred = torch.argmax(model(X), dim=1)\n",
    "            correct += sum(y_pred == y).item()\n",
    "    return correct / elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6df3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, y_pred, y_teacher, y):\n",
    "        return F.mse_loss(y_pred, y_teacher) + F.cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d8d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = Student()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75378c",
   "metadata": {},
   "source": [
    "Посмотрим теперь насколько меньше своего учителя оказался наш ученик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f57457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9564340729989932"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - get_params(student) / get_params(teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebce76",
   "metadata": {},
   "source": [
    "Ученик оказался меньше почти на 96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a33ac8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e110e3e04f4902a21bb48b535639f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss 34.76612377441406 | Test loss 23.446907876586913 | accuracy 62.47\n",
      "Epoch 2 | Train loss 21.014581605834962 | Test loss 17.864690811157228 | accuracy 72.0\n",
      "Epoch 3 | Train loss 16.294261903381347 | Test loss 14.97879337158203 | accuracy 77.09\n",
      "Epoch 4 | Train loss 13.977220952453614 | Test loss 15.412944410705567 | accuracy 76.82\n",
      "Epoch 5 | Train loss 12.182168985900878 | Test loss 13.073052536010742 | accuracy 81.76\n",
      "Epoch 6 | Train loss 10.942126379699706 | Test loss 12.813351473999024 | accuracy 82.55\n",
      "Epoch 7 | Train loss 9.730871994018555 | Test loss 12.535386804199218 | accuracy 82.58\n",
      "Epoch 8 | Train loss 8.778362272644044 | Test loss 12.294524263000488 | accuracy 83.72\n",
      "Epoch 9 | Train loss 8.105869884033202 | Test loss 12.694352851867675 | accuracy 83.1\n",
      "Epoch 10 | Train loss 7.527157604370117 | Test loss 12.675682257080078 | accuracy 82.55\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "optimizer = torch.optim.Adam(student.parameters())\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracy = []\n",
    "student.to(device)\n",
    "teacher.to(device)\n",
    "for i in tqdm(range(epochs)):\n",
    "    #Train\n",
    "    loss_mean = 0\n",
    "    elements = 0\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = student(X)\n",
    "        y_teacher = teacher(X)\n",
    "        #print('y_pred', y_pred.shape)\n",
    "        #print('y_teacher', y_teacher.shape)\n",
    "        loss = student.get_loss(y_pred, y_teacher, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_mean += loss.item() * len(X)\n",
    "        elements += len(X)\n",
    "\n",
    "    train_losses.append(loss_mean / elements)\n",
    "    #Test\n",
    "    #if (i+1) % 10 == 0:\n",
    "    loss_mean = 0 \n",
    "    elements = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in iter(test_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = student(X)\n",
    "            y_teacher = teacher(X)\n",
    "            loss = student.get_loss(y_pred, y_teacher, y)\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct += sum(y_pred == y).item()\n",
    "        accuracy.append(100 * correct / elements)\n",
    "        test_losses.append(loss_mean / elements)\n",
    "    print(\"Epoch\", i+1, \"| Train loss\", train_losses[-1], \"| Test loss\", test_losses[-1], \"| accuracy\", accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a030c34",
   "metadata": {},
   "source": [
    "Точность получилась чуть меньше, чем у лучшей модели, поэтому достану из широких штанин лучшие веса :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cf97752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.load_state_dict(torch.load('student_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b97a0562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8362"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(student, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "592f2e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8541"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(teacher, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2301cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(student.state_dict(), 'student_weights')\n",
    "#teacher.load_state_dict(torch.load('teacher_weights'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2676fe6c",
   "metadata": {},
   "source": [
    "Мы взяли в качестве учителя большую модель VGG16 с батчнормализацией. В качестве ученика мы взяли модель на 96% меньше. Потеря точности составила примерно 1.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ed6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc2782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfaa100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
